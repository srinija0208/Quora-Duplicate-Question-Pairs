# Quora Duplicate Question Detection

This repository contains multiple machine learning approaches to identify whether two questions from the **Quora Question Pairs dataset** are duplicates or not. The project explores how different **feature engineering (FE)** and **text representation techniques** impact model performance.

---

## ğŸ“Œ Problem Statement

Given a pair of questions (Q1, Q2), the goal is to predict whether they are semantically equivalent (duplicate) or not.

This is a classic **binary text classification** problem widely used to benchmark NLP pipelines.

---

## ğŸ“Š Approaches & Results

The following experiments were conducted with different feature representations and models:

| # | Features / Representation                       | Model         | Accuracy |
| - | ----------------------------------------------- | ------------- | -------- |
| 1 | Bag of Words (no preprocessing)                 | Random Forest | **75%**  |
| 2 | Preprocessing + TF-IDF                          | XGBoost       | **72%**  |
| 3 | Custom Features + Bag of Words                  | Random Forest | **77%**  |
| 4 | Custom Features + Sentence Transformer + TF-IDF | XGBoost       | **82%**  |

ğŸ“Œ **Best Performance:** Combination of **custom features + semantic embeddings + TF-IDF**

---

## ğŸ§  Feature Engineering

### ğŸ”¹ Text Preprocessing

* Lowercasing
* Punctuation removal
* Stopword removal
* URL removal
* Stemming
* Decontracting words
* Slang normalization

### ğŸ”¹ Custom Features

The following handcrafted features were used:

* `q1_len` â€“ Character length of question 1
* `q2_len` â€“ Character length of question 2
* `q1_word_count` â€“ Number of words in question 1
* `q2_word_count` â€“ Number of words in question 2
* `common_word_count` â€“ Number of common words between Q1 and Q2
* `total_word_count` â€“ Total unique words across both questions
* `word_share` â€“ Ratio of common words to total words

These features help capture **lexical similarity** beyond raw text vectors.

---

## ğŸ”¤ Text Representation Techniques

* **Bag of Words (BoW)**
* **TF-IDF Vectorization**
* **Sentence Transformers** (`all-MiniLM-L6-v2`)

Sentence embeddings help capture **semantic similarity**, which significantly improved performance.

---

## ğŸ¤– Models Used

* Random Forest Classifier
* XGBoost Classifier

---



## âš™ï¸ Tech Stack

* Python
* Pandas, NumPy
* Scikit-learn
* XGBoost
* Sentence Transformers
* NLTK / Regex

---

## ğŸš€ Key Learnings

* Preprocessing alone does not guarantee better performance
* Handcrafted features significantly boost classical ML models
* Combining **semantic embeddings with statistical features** yields the best results
* Tree-based models benefit strongly from engineered numerical features

---

## ğŸ“ˆ Future Improvements

* Fine-tune transformer models (BERT, RoBERTa)
* Use cosine similarity directly on embeddings
* Handle class imbalance with advanced sampling

---

## ğŸ™Œ Acknowledgements

* Quora Question Pairs Dataset
* HuggingFace Sentence Transformers

---

